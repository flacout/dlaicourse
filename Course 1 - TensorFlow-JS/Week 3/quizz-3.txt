1. The first is the probability value for whether or not the phrase is not an insult, and the second is the probability for whether or not it is
2. The phrase does not contain an insult
3. It returns an array of answers, each one corresponding to a different type of toxicity
4. 3
5. Tensorflowjs
6. Save it as a TensorFlow Saved Model, then use the tensorflowjs_convertor script in Python
7. const model = await tf.loadLayersModel(MODEL_URL)
8. At least two: the model file, and a sharded collection of binary weight files that can have one or more files




